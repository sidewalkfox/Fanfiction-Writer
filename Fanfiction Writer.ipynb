{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just took all of my code from the project and changed it so it works with Google Colab. Just change all of the variables you want and run the cells in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replaces saving data to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workIds = []\n",
    "works = []\n",
    "prepWork = ''\n",
    "txtOut = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'beastars' #Search tag\n",
    "requestedFics = 50 #Number of works to collect\n",
    "language = 'English' #Work language\n",
    "getExplicit = True #Get explicit works\n",
    "numGenerate = 2000 #Number of characters to write in generated work\n",
    "startString = 'The ' #First word to write in generated work\n",
    "training = True #If we are going to train a new model\n",
    "epochs = 25 #Number of epochs to generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that the program uses. Don't change these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workIds Variables\n",
    "pageEmpty = False\n",
    "baseUrl = \"\"\n",
    "url = \"https://archiveofourown.org/tags/\" + tag + \"/works\"\n",
    "recordedFics = 0\n",
    "seenIds = []\n",
    "delay = 5\n",
    "\n",
    "#Training constants\n",
    "checkpointDir = './training_checkpoints'\n",
    "seqLenth = 100\n",
    "batchSize = 64\n",
    "bufferSize = 10000\n",
    "embeddingDim = 256\n",
    "rnnUnits = 1024\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workIds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Functions\n",
    "def getIds(header_info=''):\n",
    "    global pageEmpty\n",
    "    headers = {'user-agent' : header_info}\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "    works = soup.select(\"li.work.blurb.group\")\n",
    "\n",
    "    #See if we collected all available work\n",
    "    if(len(works) == 0):\n",
    "        pageEmpty = True\n",
    "\n",
    "    #Creates list of ids\n",
    "    ids = []\n",
    "    for tag in works:\n",
    "        t = tag.get('id')\n",
    "        t = t[5:]\n",
    "        if not t in seenIds:\n",
    "            ids.append(t)\n",
    "            seenIds.append(t)\n",
    "    return ids\n",
    "\n",
    "def updateNextPage():\n",
    "    global url\n",
    "    key = \"page=\"\n",
    "    start = url.find(key)\n",
    "\n",
    "    #Checks for a page indicator\n",
    "    if(start != -1):\n",
    "        #Finds the indicator in the url\n",
    "        pageStartIndex = start + len(key)\n",
    "        pageEndIndex = url.find(\"&\", pageStartIndex)\n",
    "        #Runs if the indicator is in the middle of the url\n",
    "        if(pageEndIndex != -1):\n",
    "            page = int(url[pageStartIndex:pageEndIndex]) + 1\n",
    "            url = url[:pageStartIndex] + str(page) + url[pageEndIndex:]\n",
    "        #Runs if the indicator is at the end of the url\n",
    "        else:\n",
    "            page = int(url[pageStartIndex:]) + 1\n",
    "            url = url[:pageStartIndex] + str(page)\n",
    "\n",
    "    #Since there is no page indicator, we must be on page 1\n",
    "    else:\n",
    "        #If there are other modifiers\n",
    "        if(url.find(\"?\") != -1):\n",
    "            url = url + \"&page=2\"\n",
    "        #If there are no modifiers\n",
    "        else:\n",
    "            url = url + \"?page=2\"\n",
    "\n",
    "#Writes ids and url to .csv file\n",
    "def writeIds(ids):\n",
    "    global workIds\n",
    "    global recordedFics\n",
    "    for id in ids:\n",
    "        if(notFinished()):\n",
    "            workIds.append(id)\n",
    "            recordedFics = recordedFics + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "#Checks if we have too many files or if the page is empty\n",
    "def notFinished():\n",
    "    if(pageEmpty):\n",
    "        return False\n",
    "\n",
    "    if(requestedFics == 0):\n",
    "        return True\n",
    "    else:\n",
    "        if(recordedFics < requestedFics):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#Runs functions to get the work ids\n",
    "def processIds(headerInfo=''):\n",
    "    while(notFinished()):\n",
    "        #Delay between requests as per AO3's terms of service\n",
    "        time.sleep(delay)\n",
    "        ids = getIds(headerInfo)\n",
    "        writeIds(ids)\n",
    "        updateNextPage()\n",
    "\n",
    "#Checks if the number of requested works has been set\n",
    "if(requestedFics == 0):\n",
    "\tprint('WARNING! Number of requested works not set. Will collect all available works.\\nprocessing...')\n",
    "else:\n",
    "\tprint (\"processing...\")\n",
    "\t\n",
    "processIds()\n",
    "print(\"Finished processing\")\n",
    "print(workIds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getFanfics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Functions\n",
    "def getTagInfo(category, meta):\n",
    "\ttry:\n",
    "\t\ttagList = meta.find(\"dd\", class_=str(category) + ' tags').find_all(class_=\"tag\")\n",
    "\texcept AttributeError:\n",
    "\t\treturn []\n",
    "\treturn [result.text for result in tagList] \n",
    "\t\n",
    "#Gets information about works\n",
    "def getStats(meta):\n",
    "\t#Defines work categories\n",
    "\tcategories = ['language', 'published', 'status', 'words', 'chapters', 'comments', 'kudos', 'bookmarks', 'hits'] \n",
    "\tstats = list(map(lambda category: meta.find(\"dd\", class_=category), categories))\n",
    "\n",
    "\tif not stats[2]:\n",
    "\t\tstats[2] = stats[1]\n",
    "\ttry:\t\t\n",
    "\t\tstats = [stat.text for stat in stats]\n",
    "\texcept AttributeError as e:\n",
    "\t\tnewStats = []\n",
    "\t\tfor stat in stats:\n",
    "\t\t\tif stat: newStats.append(stat.text)\n",
    "\t\t\telse: newStats.append('null')\n",
    "\t\tstats = newStats\n",
    "\n",
    "\tstats[0] = stats[0].rstrip().lstrip()\n",
    "\tstatus = meta.find(\"dt\", class_=\"status\")\n",
    "\tif not status: status = 'Completed' \n",
    "\telse: status = status.text.strip(':')\n",
    "\tstats.insert(2, status)\n",
    "\n",
    "\treturn stats      \n",
    "\n",
    "#Defines tags and gets their value from function\n",
    "def getTags(meta):\n",
    "\ttags = ['rating', 'category', 'fandom', 'relationship', 'character', 'freeform']\n",
    "\treturn list(map(lambda tag: getTagInfo(tag, meta), tags))\n",
    "\n",
    "#Gets kudos\n",
    "def getKudos(meta):\n",
    "\tif(meta):\n",
    "\t\t#Gets kudos from work\n",
    "\t\tusers = []\n",
    "\t\tkudos = meta.contents\n",
    "\n",
    "\t\t#Extracts users from kudos variable\n",
    "\t\tfor kudo in kudos:\n",
    "\t\t\tif kudo.name == 'a':\n",
    "\t\t\t\tif 'more users' not in kudo.contents[0] and '(collapse)' not in kudo.contents[0]:\n",
    "\t\t\t\t\tusers.append(kudo.contents[0])\n",
    "\t\t\n",
    "\t\treturn users\n",
    "\treturn []\n",
    "\n",
    "#Gets bookmarks\n",
    "def getBookmarks(url, headerInfo):\n",
    "\tbookmarks = []\n",
    "\theaders = {'user-agent' : headerInfo}\n",
    "\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\tsrc = req.text\n",
    "\n",
    "\ttime.sleep(delay)\n",
    "\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "\tsys.stdout.write('Scraping bookmarks ')\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "\t#Finds all pages\n",
    "\tif(soup.find('ol', class_='pagination actions')):\n",
    "\t\tpages = soup.find('ol', class_='pagination actions').findChildren(\"li\" , recursive=False)\n",
    "\t\tmaxPages = int(pages[-2].contents[0].contents[0])\n",
    "\t\tcount = 1\n",
    "\t\n",
    "\t\tsys.stdout.write('(' + str(maxPages) + ' pages)')\n",
    "\t\tsys.stdout.flush()\n",
    "\n",
    "\t\twhile count <= maxPages:\n",
    "\t\t\t#Extracts bookmarks\n",
    "\t\t\ttags = soup.findAll('h5', class_='byline heading')\n",
    "\t\t\tbookmarks += getUsers(tags)\n",
    "\n",
    "\t\t\t#Goes to next page\n",
    "\t\t\tcount += 1\n",
    "\t\t\treq = requests.get(url+'?page='+str(count), headers=headers)\n",
    "\t\t\tsrc = req.text\n",
    "\t\t\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\t\t\tsys.stdout.write('.')\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\t\ttime.sleep(delay)\n",
    "\telse:\n",
    "\t\ttags = soup.findAll('h5', class_='byline heading')\n",
    "\t\tbookmarks += getUsers(tags)\n",
    "\n",
    "\treturn bookmarks\n",
    "\n",
    "#Gets users from meta data\n",
    "def getUsers(meta):\n",
    "\tusers = []\n",
    "\tfor tag in meta:\n",
    "\t\t\tuser = tag.findChildren(\"a\" , recursive=False)[0].contents[0]\n",
    "\t\t\tusers.append(user)\n",
    "\n",
    "\treturn users\n",
    "\t\n",
    "#Runs if the id is invalid\n",
    "def accessDenied(soup):\n",
    "\tif(soup.find(class_=\"flash error\")):\n",
    "\t\treturn True\n",
    "\tif(not soup.find(class_=\"work meta group\")):\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "#Writes all information into a csv file\n",
    "def writeCsv(ficId, language, headerInfo=''):\n",
    "\tglobal works\n",
    "\tprint('Scraping', ficId)\n",
    "\turl = 'http://archiveofourown.org/works/'+str(ficId)+'?view_adult=true'\n",
    "\theaders = {'user-agent' : headerInfo}\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\tsrc = req.text\n",
    "\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\tif(accessDenied(soup)):\n",
    "\t\tprint('Access Denied')\n",
    "\t\terrorRow = ' '.join(ficId) + ' Access Denied'\n",
    "\t\tprint('ERROR: ' + errorRow)\n",
    "\telse:\n",
    "\t\tmeta = soup.find(\"dl\", class_=\"work meta group\")\n",
    "\t\ttags = getTags(meta)\n",
    "\t\tstats = getStats(meta)\n",
    "\n",
    "\t\t#Checks if work is in the correct language\n",
    "\t\tif language != False and language != stats[0]:\n",
    "\t\t\tprint('This work is not in ' + language + ', skipping...')\n",
    "\t\telif((getExplicit == False) and (tags[0][0] == 'Explicit' or tags[0][0] == 'Mature' or tags[0][0] == 'Not Rated')):\n",
    "\t\t\tprint('This work is explicit, skiping...')\n",
    "\t\telse:\n",
    "\t\t\t#Gets the work from ao3\n",
    "\t\t\tcontent = soup.find(\"div\", id= \"chapters\")\n",
    "\t\t\tchapters = content.select('p')\n",
    "\t\t\tchaptertext = '\\n\\n'.join([chapter.text for chapter in chapters])\n",
    "\t\t\trow = [chaptertext]\n",
    "\t\t\ttry:\n",
    "\t\t\t\tworks.append(row)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint('Unexpected error: ', sys.exc_info()[0])\n",
    "\t\t\t\terrorRow = ' '.join(ficId) + ' ' + ' '.join([sys.exc_info()[0]])\n",
    "\t\t\t\tprint('ERROR: ' + errorRow)\n",
    "\t\t\tprint('Done.')\n",
    "\n",
    "#Runs functions to get the works\n",
    "for row in workIds:\n",
    "\tif not row:\n",
    "\t\tcontinue\n",
    "\twriteCsv(row, language)\n",
    "\ttime.sleep(delay)\n",
    "\n",
    "works = works[0][0]\n",
    "print('Finished scraping')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepWorks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits works into array\n",
    "works = works.split('\\n')\n",
    "\n",
    "#List of things to replace with the list below them, replaceList[0] with be replaced with replaceTerms[0]\n",
    "replaceList = ['(See the end of the chapter for  notes.)', '(See the end of the chapter for notes.)', '\",\"', '\"New work', '\"\"']\n",
    "replaceTerms = ['', '', '', '\\n\\n', '\"']\n",
    "\n",
    "#Fixes formating that applies to each line\n",
    "x=0\n",
    "for line in works:\n",
    "\t#Skips if it's the first time running, removes the first line\n",
    "\tif(x==0):\n",
    "\t\tx=x+1\n",
    "\t\tcontinue\n",
    "\t#Replace List\n",
    "\ti=0\n",
    "\tfor word in replaceList:\n",
    "\t\tline = line.replace(word, replaceTerms[i])\n",
    "\t\ti = i+1\n",
    "\t#Removes empty lines\n",
    "\tif(line.isspace()):\n",
    "\t\tline = ''\n",
    "\n",
    "\tprepWork = prepWork + line + '\\n'\n",
    "\n",
    "#Fixes formating that applies to the entire file\n",
    "prepWork = re.sub(\"\\ +\", \" \", prepWork)\n",
    "prepWork = re.sub(\"\\\\n+\", \"\\n\", prepWork)\n",
    "\n",
    "print('Finished preparing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createWork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates text file\n",
    "def generateFile(txtOut):\n",
    "\tworkName = 'epochs{}_reqFics{}'.format(epochs,requestedFics)\n",
    "\n",
    "\tfile = open(workName + '.txt', 'w')\n",
    "\tfile.write(txtOut)\n",
    "\tfile.close()\n",
    "\n",
    "#Encodes text file into utf-8\n",
    "text = prepWork\n",
    "\n",
    "#Gets list of known characters\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "#Converts readable text to machine text and vis versa\n",
    "char2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "#Converts text to a number that represents it\n",
    "textAsInt = np.array([char2idx[char] for char in text])\n",
    "charDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n",
    "sequences = charDataset.batch(seqLenth+1, drop_remainder=True)\n",
    "\n",
    "def splitInputTarget(chunk):\n",
    "\tinputText = chunk[:-1]\n",
    "\ttargetText = chunk[1:]\n",
    "\treturn inputText, targetText\n",
    "\n",
    "dataset = sequences.map(splitInputTarget)\n",
    "dataset = dataset.shuffle(bufferSize).batch(batchSize, drop_remainder=True)\n",
    "\n",
    "vocabSize = len(vocab)\n",
    "\n",
    "#Prepares the training of a model. This can train a new model or load a previously trained one\n",
    "def buildModel(vocabSize, embeddingDim, rnnUnits, batchSize):\n",
    "\tmodel = tf.keras.Sequential([\n",
    "\t\ttf.keras.layers.Embedding(vocabSize, embeddingDim, batch_input_shape=[batchSize, None]),\n",
    "\t\ttf.keras.layers.GRU(rnnUnits, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "\t\ttf.keras.layers.Dense(vocabSize)\n",
    "\t])\n",
    "\treturn model\n",
    "\n",
    "#This will run when the program is creating a trained model\n",
    "if(training == True):\n",
    "\tmodel = buildModel(vocabSize=len(vocab), embeddingDim=embeddingDim, rnnUnits=rnnUnits, batchSize=batchSize)\n",
    "\n",
    "\tdef loss(labels, logits):\n",
    "\t\treturn tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\tmodel.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "\tcheckpointPrefix = os.path.join(checkpointDir, 'chkpt_{epoch}')\n",
    "\tcheckpointCallback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointPrefix, save_weights_only=True)\n",
    "\n",
    "\thistory = model.fit(dataset, epochs=epochs, callbacks=[checkpointCallback])\n",
    "\n",
    "#Collects information about the trained model and presents them\n",
    "model = buildModel(vocabSize, embeddingDim, rnnUnits, batchSize=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpointDir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()\n",
    "\n",
    "#Final code that generates the output work\n",
    "def generateText(model, startString):\n",
    "\tinputEval = [char2idx[s] for s in startString]\n",
    "\tinputEval = tf.expand_dims(inputEval, 0)\n",
    "\n",
    "\ttextGenerated = []\n",
    "\n",
    "\tmodel.reset_states()\n",
    "\tfor i in range(numGenerate):\n",
    "\t\tpredictions = model(inputEval)\n",
    "\t\tpredictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "\t\t#Predictions get crazier as temperature goes up\n",
    "\t\tpredictions = predictions / temperature\n",
    "\t\tpredictedId = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "\t\tinputEval = tf.expand_dims([predictedId], 0)\n",
    "\t\ttextGenerated.append(idx2char[predictedId])\n",
    "\n",
    "\treturn (startString + ''.join(textGenerated))\n",
    "\n",
    "#Gets generated text and runs the save function\n",
    "txtOut = generateText(model, startString=startString)\n",
    "generateFile(txtOut)\n",
    "print('Finished generating work')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f66ac1c90b45b7e8f58e34a684950ef450ac6b5af5544ee5bf012ac10dd7bea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
