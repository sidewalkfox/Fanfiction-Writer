{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just took all of my code from the project and changed it so it works with Google Colab. Just change all of the variables you want and run the cells in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'furry' #Search term\n",
    "requestedFics = 5 #Number of works to be collected\n",
    "language = 'English' #Work language\n",
    "getExplicit = True #If we will collect explicit works\n",
    "training = True #If we are going to train a new model\n",
    "numGenerate = 2000 #Number of characters the finished work will have\n",
    "epochs = 25 #Number of epochs to generate\n",
    "startString = 'The ' #First word of the generated work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workIds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "#Variables\n",
    "pageEmpty = False\n",
    "baseUrl = \"\"\n",
    "url = \"https://archiveofourown.org/tags/\" + tag + \"/works\"\n",
    "recordedFics = 0\n",
    "csvName = \"workIds\"\n",
    "\n",
    "#Keeps track of all work ids to not repeat\n",
    "seenIds = []\n",
    "\n",
    "#Must be over 5\n",
    "delay = 5\n",
    "\n",
    "##Functions\n",
    "def getIds(header_info=''):\n",
    "    global pageEmpty\n",
    "    headers = {'user-agent' : header_info}\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "    works = soup.select(\"li.work.blurb.group\")\n",
    "\n",
    "    #See if we collected all available work\n",
    "    if(len(works) == 0):\n",
    "        pageEmpty = True\n",
    "\n",
    "    #Creates list of ids\n",
    "    ids = []\n",
    "    for tag in works:\n",
    "        t = tag.get('id')\n",
    "        t = t[5:]\n",
    "        if not t in seenIds:\n",
    "            ids.append(t)\n",
    "            seenIds.append(t)\n",
    "    return ids\n",
    "\n",
    "def updateNextPage():\n",
    "    global url\n",
    "    key = \"page=\"\n",
    "    start = url.find(key)\n",
    "\n",
    "    #Checks for a page indicator\n",
    "    if(start != -1):\n",
    "        #Finds the indicator in the url\n",
    "        pageStartIndex = start + len(key)\n",
    "        pageEndIndex = url.find(\"&\", pageStartIndex)\n",
    "        #Runs if the indicator is in the middle of the url\n",
    "        if(pageEndIndex != -1):\n",
    "            page = int(url[pageStartIndex:pageEndIndex]) + 1\n",
    "            url = url[:pageStartIndex] + str(page) + url[pageEndIndex:]\n",
    "        #Runs if the indicator is at the end of the url\n",
    "        else:\n",
    "            page = int(url[pageStartIndex:]) + 1\n",
    "            url = url[:pageStartIndex] + str(page)\n",
    "\n",
    "    #Since there is no page indicator, we must be on page 1\n",
    "    else:\n",
    "        #If there are other modifiers\n",
    "        if(url.find(\"?\") != -1):\n",
    "            url = url + \"&page=2\"\n",
    "        #If there are no modifiers\n",
    "        else:\n",
    "            url = url + \"?page=2\"\n",
    "\n",
    "#Writes ids and url to .csv file\n",
    "def writeIds(ids):\n",
    "    global recordedFics\n",
    "    with open(csvName + \".csv\", 'a') as csvfile:\n",
    "        wr = csv.writer(csvfile, delimiter=',')\n",
    "        for id in ids:\n",
    "            if(notFinished()):\n",
    "                wr.writerow([id, url])\n",
    "                recordedFics = recordedFics + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "#Checks if we have too many files or if the page is empty\n",
    "def notFinished():\n",
    "    if(pageEmpty):\n",
    "        return False\n",
    "\n",
    "    if(requestedFics == 0):\n",
    "        return True\n",
    "    else:\n",
    "        if(recordedFics < requestedFics):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#Runs functions to get the work ids\n",
    "def processIds(headerInfo=''):\n",
    "    while(notFinished()):\n",
    "        #Delay between requests as per AO3's terms of service\n",
    "        time.sleep(delay)\n",
    "        ids = getIds(headerInfo)\n",
    "        writeIds(ids)\n",
    "        updateNextPage()\n",
    "\n",
    "#Gets called to start the program\n",
    "def main():\n",
    "    #Clears the workIds file\n",
    "    idFile = open(csvName + \".csv\", \"w\")\n",
    "    idFile.truncate()\n",
    "    idFile.close()\n",
    "\n",
    "    #Checks if the number of requested works has been set\n",
    "    if(requestedFics == 0):\n",
    "        print('WARNING! Number of requested works not set. Will collect all available works.\\nprocessing...')\n",
    "    else:\n",
    "        print (\"processing...\")\n",
    "        \n",
    "    processIds()\n",
    "    print (\"Finished processing\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getFanfics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#Variables\n",
    "ficIds = [csvName + '.csv']\n",
    "csvOut = 'works.txt'\n",
    "\n",
    "##Functions\n",
    "def getTagInfo(category, meta):\n",
    "\ttry:\n",
    "\t\ttagList = meta.find(\"dd\", class_=str(category) + ' tags').find_all(class_=\"tag\")\n",
    "\texcept AttributeError:\n",
    "\t\treturn []\n",
    "\treturn [result.text for result in tagList] \n",
    "\t\n",
    "#Gets information about works\n",
    "def getStats(meta):\n",
    "\t#Defines work categories\n",
    "\tcategories = ['language', 'published', 'status', 'words', 'chapters', 'comments', 'kudos', 'bookmarks', 'hits'] \n",
    "\tstats = list(map(lambda category: meta.find(\"dd\", class_=category), categories))\n",
    "\n",
    "\tif not stats[2]:\n",
    "\t\tstats[2] = stats[1]\n",
    "\ttry:\t\t\n",
    "\t\tstats = [stat.text for stat in stats]\n",
    "\texcept AttributeError as e:\n",
    "\t\tnewStats = []\n",
    "\t\tfor stat in stats:\n",
    "\t\t\tif stat: newStats.append(stat.text)\n",
    "\t\t\telse: newStats.append('null')\n",
    "\t\tstats = newStats\n",
    "\n",
    "\tstats[0] = stats[0].rstrip().lstrip()\n",
    "\tstatus = meta.find(\"dt\", class_=\"status\")\n",
    "\tif not status: status = 'Completed' \n",
    "\telse: status = status.text.strip(':')\n",
    "\tstats.insert(2, status)\n",
    "\n",
    "\treturn stats      \n",
    "\n",
    "#Defines tags and gets their value from function\n",
    "def getTags(meta):\n",
    "\ttags = ['rating', 'category', 'fandom', 'relationship', 'character', 'freeform']\n",
    "\treturn list(map(lambda tag: getTagInfo(tag, meta), tags))\n",
    "\n",
    "#Gets kudos\n",
    "def getKudos(meta):\n",
    "\tif(meta):\n",
    "\t\t#Gets kudos from work\n",
    "\t\tusers = []\n",
    "\t\tkudos = meta.contents\n",
    "\n",
    "\t\t#Extracts users from kudos variable\n",
    "\t\tfor kudo in kudos:\n",
    "\t\t\tif kudo.name == 'a':\n",
    "\t\t\t\tif 'more users' not in kudo.contents[0] and '(collapse)' not in kudo.contents[0]:\n",
    "\t\t\t\t\tusers.append(kudo.contents[0])\n",
    "\t\t\n",
    "\t\treturn users\n",
    "\treturn []\n",
    "\n",
    "#Gets bookmarks\n",
    "def getBookmarks(url, headerInfo):\n",
    "\tbookmarks = []\n",
    "\theaders = {'user-agent' : headerInfo}\n",
    "\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\tsrc = req.text\n",
    "\n",
    "\ttime.sleep(delay)\n",
    "\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "\tsys.stdout.write('Scraping bookmarks ')\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "\t#Finds all pages\n",
    "\tif(soup.find('ol', class_='pagination actions')):\n",
    "\t\tpages = soup.find('ol', class_='pagination actions').findChildren(\"li\" , recursive=False)\n",
    "\t\tmaxPages = int(pages[-2].contents[0].contents[0])\n",
    "\t\tcount = 1\n",
    "\t\n",
    "\t\tsys.stdout.write('(' + str(maxPages) + ' pages)')\n",
    "\t\tsys.stdout.flush()\n",
    "\n",
    "\t\twhile count <= maxPages:\n",
    "\t\t\t#Extracts bookmarks\n",
    "\t\t\ttags = soup.findAll('h5', class_='byline heading')\n",
    "\t\t\tbookmarks += getUsers(tags)\n",
    "\n",
    "\t\t\t#Goes to next page\n",
    "\t\t\tcount += 1\n",
    "\t\t\treq = requests.get(url+'?page='+str(count), headers=headers)\n",
    "\t\t\tsrc = req.text\n",
    "\t\t\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\t\t\tsys.stdout.write('.')\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\t\ttime.sleep(delay)\n",
    "\telse:\n",
    "\t\ttags = soup.findAll('h5', class_='byline heading')\n",
    "\t\tbookmarks += getUsers(tags)\n",
    "\n",
    "\treturn bookmarks\n",
    "\n",
    "#Gets users from meta data\n",
    "def getUsers(meta):\n",
    "\tusers = []\n",
    "\tfor tag in meta:\n",
    "\t\t\tuser = tag.findChildren(\"a\" , recursive=False)[0].contents[0]\n",
    "\t\t\tusers.append(user)\n",
    "\n",
    "\treturn users\n",
    "\t\n",
    "#Runs if the id is invalid\n",
    "def accessDenied(soup):\n",
    "\tif(soup.find(class_=\"flash error\")):\n",
    "\t\treturn True\n",
    "\tif(not soup.find(class_=\"work meta group\")):\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "#Writes all information into a csv file\n",
    "def writeCsv(ficId, language, writer, headerInfo=''):\n",
    "\tprint('Scraping', ficId)\n",
    "\turl = 'http://archiveofourown.org/works/'+str(ficId)+'?view_adult=true'\n",
    "\theaders = {'user-agent' : headerInfo}\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\tsrc = req.text\n",
    "\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\tif(accessDenied(soup)):\n",
    "\t\tprint('Access Denied')\n",
    "\t\terrorRow = ' '.join(ficId) + ' Access Denied'\n",
    "\t\tprint('ERROR: ' + errorRow)\n",
    "\telse:\n",
    "\t\tmeta = soup.find(\"dl\", class_=\"work meta group\")\n",
    "\t\ttags = getTags(meta)\n",
    "\t\tstats = getStats(meta)\n",
    "\n",
    "\t\t#Checks if work is in the correct language\n",
    "\t\tif language != False and language != stats[0]:\n",
    "\t\t\tprint('This work is not in ' + language + ', skipping...')\n",
    "\t\telif((getExplicit == False) and (tags[0][0] == 'Explicit' or tags[0][0] == 'Mature' or tags[0][0] == 'Not Rated')):\n",
    "\t\t\tprint('This work is explicit, skiping...')\n",
    "\t\telse:\n",
    "\t\t\t#Gets the work from ao3\n",
    "\t\t\tcontent = soup.find(\"div\", id= \"chapters\")\n",
    "\t\t\tchapters = content.select('p')\n",
    "\t\t\tchaptertext = '\\n\\n'.join([chapter.text for chapter in chapters])\n",
    "\t\t\trow = [chaptertext]\n",
    "\t\t\ttry:\n",
    "\t\t\t\twriter.writerow(row)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint('Unexpected error: ', sys.exc_info()[0])\n",
    "\t\t\t\terrorRow = ' '.join(ficId) + ' ' + ' '.join([sys.exc_info()[0]])\n",
    "\t\t\t\tprint('ERROR: ' + errorRow)\n",
    "\t\t\tprint('Done.')\n",
    "\n",
    "#This is called to start the program\n",
    "def main():\n",
    "\tos.chdir(os.getcwd())\n",
    "\twith open(csvOut, 'w') as f_out:\n",
    "\t\twriter = csv.writer(f_out)\n",
    "\t\tcsvFname = ficIds[0]\n",
    "\t\twith open(csvFname, 'r+') as f_in:\n",
    "\t\t\treader = csv.reader(f_in)\n",
    "\t\t\tfor row in reader:\n",
    "\t\t\t\tif not row:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\twriteCsv(row[0], language, writer)\n",
    "\t\t\t\ttime.sleep(delay)\n",
    "\t\t\t\n",
    "\t\t\tprint('Finished scraping')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepWorks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Variables\n",
    "textFile = csvOut\n",
    "textOut = 'prepWork.txt'\n",
    "textContent = ''\n",
    "\n",
    "##Functions\n",
    "def main():\n",
    "\tglobal textContent\n",
    "\t#List of things to replace with the list below them, replaceList[0] with be replaced with replaceTerms[0]\n",
    "\treplaceList = ['(See the end of the chapter for  notes.)', '(See the end of the chapter for notes.)', '\",\"', '\"New work', '\"\"']\n",
    "\treplaceTerms = ['', '', '', '\\n\\n', '\"']\n",
    "\n",
    "\t#Opens both text files\n",
    "\twith open(textFile) as txtIn, open(textOut, 'w+') as txtOut:\n",
    "\t\t#Fixes formating that applies to each line\n",
    "\t\tx=0\n",
    "\t\tfor line in txtIn:\n",
    "\t\t\t#Skips if it's the first time running, removes the first line\n",
    "\t\t\tif(x==0):\n",
    "\t\t\t\tx=x+1\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t#Replace List\n",
    "\t\t\ti=0\n",
    "\t\t\tfor word in replaceList:\n",
    "\t\t\t\tline = line.replace(word, replaceTerms[i])\n",
    "\t\t\t\ti = i+1\n",
    "\t\t\t#Removes empty lines\n",
    "\t\t\tif(line.isspace()):\n",
    "\t\t\t\tline = ''\n",
    "\n",
    "\t\t\ttextContent = textContent + line\n",
    "\n",
    "\t\t#Fixes formating that applies to the entire file\n",
    "\t\ttextContent = re.sub(\"\\ +\", \" \", textContent)\n",
    "\t\ttextContent = re.sub(\"\\\\n+\", \"\\n\\n\", textContent)\n",
    "\t\ttxtOut.write(textContent)\n",
    "\t\tprint('Finished preparing')\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createWork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "##Variables\n",
    "#File location of the loaded text converted to absolute path\n",
    "prepWork = os.path.abspath(textOut)\n",
    "\n",
    "#Training constants\n",
    "checkpointDir = './training_checkpoints'\n",
    "seqLenth = 100\n",
    "batchSize = 64\n",
    "bufferSize = 10000\n",
    "embeddingDim = 256\n",
    "rnnUnits = 1024\n",
    "temperature = 1.0\n",
    "\n",
    "#Finished work name\n",
    "try:\n",
    "\treqFics = sum(1 for line in open(csvName + '.csv')) // 2\n",
    "except:\n",
    "\treqFics = csvName + '.csv'\n",
    "\n",
    "##Functions\n",
    "#Saves the generated text into a file\n",
    "def genFile(text):\n",
    "\tworkName = 'epochs{}_reqFics{}'.format(epochs,reqFics)\n",
    "\n",
    "\ttxtOut = open(workName + '.txt', 'w')\n",
    "\ttxtOut.write(text)\n",
    "\ttxtOut.close()\n",
    "\n",
    "#This is called to start the program\n",
    "def main():\n",
    "\t#Encodes text file into utf-8\n",
    "\ttext = open(prepWork, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "\t#Gets list of known characters\n",
    "\tvocab = sorted(set(text))\n",
    "\tprint('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "\t#Converts readable text to machine text and vis versa\n",
    "\tchar2idx = {unique:idx for idx, unique in enumerate(vocab)}\n",
    "\tidx2char = np.array(vocab)\n",
    "\n",
    "\t#Converts text to a number that represents it\n",
    "\ttextAsInt = np.array([char2idx[char] for char in text])\n",
    "\tcharDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n",
    "\tsequences = charDataset.batch(seqLenth+1, drop_remainder=True)\n",
    "\n",
    "\tdef splitInputTarget(chunk):\n",
    "\t\tinputText = chunk[:-1]\n",
    "\t\ttargetText = chunk[1:]\n",
    "\t\treturn inputText, targetText\n",
    "\n",
    "\tdataset = sequences.map(splitInputTarget)\n",
    "\tdataset = dataset.shuffle(bufferSize).batch(batchSize, drop_remainder=True)\n",
    "\n",
    "\tvocabSize = len(vocab)\n",
    "\n",
    "\t#Prepares the training of a model. This can train a new model or load a previously trained one\n",
    "\tdef buildModel(vocabSize, embeddingDim, rnnUnits, batchSize):\n",
    "\t\tmodel = tf.keras.Sequential([\n",
    "\t\t\ttf.keras.layers.Embedding(vocabSize, embeddingDim, batch_input_shape=[batchSize, None]),\n",
    "\t\t\ttf.keras.layers.GRU(rnnUnits, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "\t\t\ttf.keras.layers.Dense(vocabSize)\n",
    "\t\t])\n",
    "\t\treturn model\n",
    "\n",
    "\t#This will run when the program is creating a trained model\n",
    "\tif(training == True):\n",
    "\t\tmodel = buildModel(vocabSize=len(vocab), embeddingDim=embeddingDim, rnnUnits=rnnUnits, batchSize=batchSize)\n",
    "\n",
    "\t\tdef loss(labels, logits):\n",
    "\t\t\treturn tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\t\tmodel.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "\t\tcheckpointPrefix = os.path.join(checkpointDir, 'chkpt_{epoch}')\n",
    "\t\tcheckpointCallback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointPrefix, save_weights_only=True)\n",
    "\n",
    "\t\thistory = model.fit(dataset, epochs=epochs, callbacks=[checkpointCallback])\n",
    "\n",
    "\t#Collects information about the trained model and presents them\n",
    "\tmodel = buildModel(vocabSize, embeddingDim, rnnUnits, batchSize=1)\n",
    "\tmodel.load_weights(tf.train.latest_checkpoint(checkpointDir))\n",
    "\tmodel.build(tf.TensorShape([1, None]))\n",
    "\tmodel.summary()\n",
    "\n",
    "\t#Final code that generates the output work\n",
    "\tdef generateText(model, startString):\n",
    "\t\tinputEval = [char2idx[s] for s in startString]\n",
    "\t\tinputEval = tf.expand_dims(inputEval, 0)\n",
    "\n",
    "\t\ttextGenerated = []\n",
    "\n",
    "\t\tmodel.reset_states()\n",
    "\t\tfor i in range(numGenerate):\n",
    "\t\t\tpredictions = model(inputEval)\n",
    "\t\t\tpredictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "\t\t\t#Predictions get crazier as temperature goes up\n",
    "\t\t\tpredictions = predictions / temperature\n",
    "\t\t\tpredictedId = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "\t\t\tinputEval = tf.expand_dims([predictedId], 0)\n",
    "\t\t\ttextGenerated.append(idx2char[predictedId])\n",
    "\n",
    "\t\treturn (startString + ''.join(textGenerated))\n",
    "\n",
    "\t#Gets generated text and runs the save function\n",
    "\tgenText = generateText(model, startString=startString)\n",
    "\tgenFile(genText)\n",
    "\tprint('Finished generating work')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f66ac1c90b45b7e8f58e34a684950ef450ac6b5af5544ee5bf012ac10dd7bea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
